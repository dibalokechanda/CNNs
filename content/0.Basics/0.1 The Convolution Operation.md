Depending on your background you may have different interpretation of the convolution operation. However, the convolution operation is fundamentally different in convolutional neural networks which often causes confusion. Below I clarify some of these confusions:

### Convolution or Cross-Correlation ?

If you come from the signal processing world, you will know the kernel in the convolution operation is flipped before the convolution operation. If you do not flip the kernel it has a different name "Cross-Correlation". 

In convolutional neural network, we actually perform cross-correlation instead of convolution because the kernel is not flipped.

### Review of 1D convolution

>Note that, all convolution operation (actually cross-correlation) in this blog is discrete in nature. Also, rather than using the word "signal", the word "feature" is used which is the common terminology in the ML world.

-------------

In the following diagram each `cube` represents a single scalar value.  The input feature is shown in blue, the convolutional kernel is shown in orange and the resultant 1D output feature is shown in green.

![[Pasted image 20240626131134.png]]

Look at the following diagram which summarizes the main concept behind how convolution operation happens.

![[Pasted image 20240626123635.png]]

We take the 1D convolutional kernel (which is 1x3 dimensional and colored in orange ) and align it with a certain section of the 1D feature. For the ease of visualization I showed that certain section separately (colored in purple). Also, to explicitly intuit how the alignment works, I used a darker color to indicate the center of the kernel and the feature section that overlaps with the kernel. The next step is performing an element-wise multiplication followed by a summation operation.

The following is a simple animation to show the overall process.

![[Presentation3.mp4]]

> [!Bias Term]
> For convivence, I did not mention the bias term that is added at the end. 

### 2D Convolution vs Conv2D

Extrapolating the idea of 1D convolution to 2D convolution is pretty easy.

It should look something like below; where the instead of 1D feature, the feature is now a 2D grid. Additionally the kernel is also a 2D grid. 

![[Pasted image 20240626134407.png]]

In this visualization the kernel is  3x3 in dimension. A nice collection of these 2D convolutions can found in this [github repo](https://github.com/vdumoulin/conv_arithmetic) by Vincent Dumoulin, Francesco Visin.

However, this 2D convolution is fundamentally different from how convolution works in CNNs. In CNN we have an additional dimension for features. Hence, rather than a 2D grid of features, it is a feature volume. This means the feature has three dimensions which are respectively  denoted by $H$, $W$ and $C$.  Hence, the shape of the feature volume is  $C \times H \times W$ .

![[Pasted image 20240626141009.png]]

This means the convolutional kernel also have to have a 3D shape. In the diagram above I show the convolutional kernel which has a shape of  $C \times K \times K$; where $K=3$.

## Why it is Conv2D not Conv3D ?

One common question that arises when you are introduced to Conv2D is  "Why this still called a 2D convolution even though everything is in 3D?"
