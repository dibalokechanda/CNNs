Depending on your background you may have different interpretations of the convolution operation. However, the convolution operation is fundamentally different in convolutional neural networks which often causes confusion. Below I clarify some of these confusions:

### Convolution or Cross-Correlation?

If you come from the signal processing world, you will know the kernel in the convolution operation is flipped before the convolution operation. If you do not flip the kernel it has a different name "Cross-Correlation". 

In a convolutional neural network, we actually perform cross-correlation instead of convolution because the kernel is not flipped.

### Review of 1D convolution

>Note that, all convolution operation (actually cross-correlation) in this blog is discrete in nature. Also, rather than using the word "signal", the word "feature" is used which is the common terminology in the ML world.

-------------

In the following diagram, each `cube` represents a single scalar value.  The input feature is shown in blue, the convolutional kernel is shown in orange and the resultant 1D output feature is shown in green.

![[Pasted image 20240626131134.png]]

Look at the following diagram which summarizes the main concept behind how convolution operation happens.

![[Pasted image 20240626235400.png]]

We take the 1D convolutional kernel (which is 1x3 dimensional and colored in orange ) and align it with a certain section of the 1D feature. For the ease of visualization I showed that certain section separately (colored in purple). Also, to explicitly intuit how the alignment works, I used a darker color to indicate the center of the kernel and the feature section that overlaps with the kernel. The next step is performing an element-wise multiplication followed by a summation operation.

A complete visualization is shown below:

![[1D.mp4]]

> [!Bias Term]
> For convenience, I did not mention the bias term that is added at the end. 

### 2D Convolution vs Conv2D

Extrapolating the idea of 1D convolution to 2D convolution is pretty easy.

It should look something like below; where instead of 1D feature, the feature is now a 2D grid. Additionally, the kernel is also a 2D grid. 

![[Pasted image 20240627000256.png]]

In this visualization, the kernel is 3x3 in dimension and it is moved along the 2D feature grid which results in a transformed 2D feature grid. A nice collection of these 2D convolutions can be found in this [github repo](https://github.com/vdumoulin/conv_arithmetic) by Vincent Dumoulin, Francesco Visin.

However, this 2D convolution is fundamentally different from how convolution works in CNNs. In CNN we have an additional dimension for features. Hence, rather than a 2D grid of features, it is a feature volume. This means the feature has three dimensions which are respectively denoted by $H$, $W$ and $C$.  Hence, the shape of the feature volume is  $C \times H \times W$.

![[Pasted image 20240627001237.png]]

This means the convolutional kernel also has to have a 3D shape. In the diagram above I show the convolutional kernel which has a shape of  $C \times K \times K$; where $K=3$.

![[2D.mp4]]
### Why it is Conv2D?

One common question that arises when you are introduced to Conv2D is  "Why is this still called a 2D convolution even though everything is in 3D? Shouldn't this be a 3D convolution?" 

This is still a 2D convolution, because the kernel can only be moved along the height and width dimension, it can not be moved in the feature dimension (along the $C$ dimension).

> [!tip]
> The kernel depth is always the same as the feature volume depth.

On the other hand, 3D convolution will look like below:

![[Pasted image 20240627031058.png]]
The exact same idea continues to hold, however one distinct difference is the convolutional kernel can be also be moved along the third dimension. A visualization (partial animation) is shown below. You will notice the resultant thing after the convolution is also a 3D feature volume, not a 2D feature grid.  

![[3D_new.mp4]]


