When convolution operation is discussed, often the batch dimension is ignored for simplicity. One of the reason is during architecture implementation (in code), we almost never have to worry about the batch dimension. 

![[Pasted image 20240628195330.png]]

Consider the batch size $N$ as shown in the diagram above. This means the input is a not a single feature volume but $N$ feature volumes. Also, output also consists of $N$ feature volume. Because of this the true shape of the input tensor is $N\times C_{in}\times H_{in} \times W_{in}$ and for output tensor  $N\times C_{out}\times H_{out} \times W_{out}$




